{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** YANG Yongze\n",
    "\n",
    "**EID:** 58162280\n",
    "\n",
    "**Kaggle Team Name:** YANG Yongze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5489 - Assignment 1 - SMS Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Final submission\n",
    "In this file, put the code that generates your final Kaggle submission. It will be used to verify that your Kaggle submission is reproducible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yyz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yyz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\yyz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\yyz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2985\n",
      "2986\n",
      "Preprocessing finished.\n",
      "CountVectorizer feature representing finished.\n",
      "TF-IDF feature representing calculated.\n",
      "BernoulliNB Trained.\n",
      "Predicting finished\n",
      "[normal]: Dunno da next show aft 6 is 850. Toa payoh got 650.\n",
      "[normal]: I.ll hand her my phone to chat wit u\n",
      "[normal]: I dont have i shall buy one dear\n",
      "[normal]: Nite...\n",
      "[normal]: Ok�congrats�\n",
      "[normal]: Oops I did have it,  &lt;#&gt; ?\n",
      "[normal]: Yeah. I got a list with only u and Joanna if I'm feeling really anti social\n",
      "[normal]: Message from . I am at Truro Hospital on ext. You can phone me here. as I have a phone by my side\n",
      "[normal]: Y ü wan to go there? C doctor?\n",
      "[normal]: Not from this campus. Are you in the library?\n",
      "[spam]: I'd like to tell you my deepest darkest fantasies. Call me 09094646631 just 60p/min. To stop texts call 08712460324 (nat rate)\n",
      "[spam]: Santa Calling! Would your little ones like a call from Santa Xmas eve? Call 09058094583 to book your time.\n",
      "[spam]: Meet Top 35 US universities in Delhi at India Habitat Centre Lodhi Road on Nov 8th, 2 to 6 pm for student admission.Entry Free,  details contact 9911489000\n",
      "[spam]: SMS AUCTION You have won a Nokia 7250i. This is what you get when you win our FREE auction. To take part send Nokia to 86021 now. HG/Suite342/2Lands Row/W1JHL 16+\n",
      "[spam]: Call Germany for only 1 pence per minute! Call from a fixed line via access number 0844 861 85 86.\n",
      "[spam]: Dear Subscriber ur draw 4 £100 gift voucher will b entered on receipt of a correct ans. When was Elvis Presleys Birthday? TXT answer to 80062\n",
      "[spam]: Property Bhi Aapki Rent Bhi Aapka, Invest Rs 17k Lacs in Fully Furnished Office Spaces & Earn Rs 17k per Month Wid Bank Guarantee, Noida Cont.9990584817\n",
      "[spam]: For ur chance to win a £250 wkly shopping spree TXT: SHOP to 80878. T's&C's www.txt-2-shop.com\n",
      "[spam]: Monthly password for wap. mobsi.com is 391784. Use your wap phone not PC.\n",
      "[spam]: lam Frederick banner. Pls conatct me asap regarding a possible late\n",
      "[smishing]: WIN URGENT! Your mobile number has been awarded with a £2000 prize GUARANTEED call 09061790121 from land line. claim 3030 valid 12hrs only 150ppm\n",
      "[smishing]: Customer service annoncement. You have a New Years delivery waiting for you. Please call 07046744435 now to arrange delivery\n",
      "[smishing]: Todays Vodafone numbers ending 9167 are selected to receive a Rs.2,00,000 award. If you have a match please call 7044518857 quoting claim code 5001 standard rates apply\n",
      "[smishing]: \tFree 1st week entry 2 TEXTPOD 4 a chance 2 win 40GB iPod or £250 cash every wk. Txt VPOD to 81303 Ts&Cs www.textpod.net custcare 08712405020.\n",
      "[smishing]: 449050000301 You have won a £2,000 price! To claim, call 09050000301.\n",
      "[smishing]: tddnewsletter@emc1.co.uk (More games from TheDailyDraw) Dear Helen, Dozens of Free Games - with great prizesWith..\n",
      "[smishing]: \tDear Voucher Holder, 2 claim this weeks offer, at your PC go to http://www.e-tlp.co.uk/expressoffer Ts&Cs app\n",
      "[smishing]: You are being contacted by our Dating Service by someone you know! To find out who it is, call from your mobile or landline 09064017305 PoBox75LDNS7 \n",
      "[smishing]: Camera - You are awarded a SiPix Digital Camera! call 09061221065 fromm landline. Delivery within 8 days\n",
      "[smishing]: \tUR GOING 2 BAHAMAS! CallFREEFONE 08081560665 and speak to a live operator to claim either Bahamas cruise of£2000 CASH 18+only. To opt out txt X to 07786200117\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib_inline   # setup output image format\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "from scipy import stats\n",
    "import csv\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "random.seed(100)\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Init stop words and stemmer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define some helper functions\n",
    "def read_text_data(fname):\n",
    "    txtdata = []\n",
    "    classes = []\n",
    "    with open(fname, 'r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "        for row in reader:\n",
    "            # get the text\n",
    "            txtdata.append(row[0])\n",
    "            # get the class (convert to integer)\n",
    "            if len(row)>1:\n",
    "                classes.append(int(row[1]))\n",
    "\n",
    "    return (txtdata, classes)\n",
    "\n",
    "def write_csv_kaggle_sub(fname, Y):\n",
    "    # fname = file name\n",
    "    # Y is a list/array with class entries\n",
    "\n",
    "    # header\n",
    "    tmp = [['Id', 'Prediction']]\n",
    "\n",
    "    # add ID numbers for each Y\n",
    "    for (i,y) in enumerate(Y):\n",
    "        tmp2 = [(i+1), y]\n",
    "        tmp.append(tmp2)\n",
    "\n",
    "    # write CSV file\n",
    "    with open(fname, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(tmp)\n",
    "\n",
    "def preprocess(str):\n",
    "    \"\"\"\n",
    "    Preprocess text data by removing non-alphanumeric characters & stop word and lemmatizing\n",
    "\n",
    "    :param str: input string\n",
    "    :return: preprocessed string\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove non-alphanumeric characters\n",
    "    string_only_alphanumeric = re.sub(r'[^a-zA-Z0-9]', ' ', str)\n",
    "    # Tokenize the string into individual words\n",
    "    words = word_tokenize(string_only_alphanumeric)\n",
    "    new_str = ''\n",
    "\n",
    "    for word in words:\n",
    "        if not word.isdigit():\n",
    "            # Remove stop word\n",
    "            if word not in stop_words:\n",
    "                # lemmatization and convert to lower case\n",
    "                stemmed_word = lemmatizer.lemmatize(word)\n",
    "                lower_word = stemmed_word.lower()\n",
    "                if new_str == '':\n",
    "                    new_str += lower_word\n",
    "                else:\n",
    "                    new_str = new_str + ' ' + lower_word\n",
    "    return new_str\n",
    "\n",
    "# load the data\n",
    "(traintxt, trainY) = read_text_data(\"smishing_train.txt\")\n",
    "(testtxt, _)       = read_text_data(\"smishing_test.txt\")\n",
    "\n",
    "print(len(traintxt))\n",
    "print(len(testtxt))\n",
    "\n",
    "# Preprocess the data\n",
    "train_txt_preprocessed = [preprocess(s) for s in traintxt]\n",
    "test_txt_preprocessed = [preprocess(s) for s in testtxt]\n",
    "print('Preprocessing finished.')\n",
    "\n",
    "# Count Vectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=3150)\n",
    "train_features = vectorizer.fit_transform(train_txt_preprocessed)\n",
    "test_features = vectorizer.transform(test_txt_preprocessed)\n",
    "print('CountVectorizer feature representing finished.')\n",
    "\n",
    "# Calculate TF-IDF value\n",
    "tf_trans = feature_extraction.text.TfidfTransformer(use_idf=True, norm='l1')\n",
    "train_X = tf_trans.fit_transform(train_features)\n",
    "test_X = tf_trans.transform(test_features)\n",
    "print('TF-IDF feature representing calculated.')\n",
    "\n",
    "# Train BernoulliNB\n",
    "model_bnb = naive_bayes.BernoulliNB(alpha=0.1)\n",
    "model_bnb.fit(train_X,trainY)\n",
    "print('BernoulliNB Trained.')\n",
    "\n",
    "y_pred_bnb = model_bnb.predict(test_X)\n",
    "print('Predicting finished')\n",
    "\n",
    "# Make the Submission CSV file\n",
    "write_csv_kaggle_sub(\"final_submission.csv\", y_pred_bnb)\n",
    "# See some predicted data\n",
    "classnames = unique(trainY)\n",
    "classlabels = ['normal', 'spam', 'smishing']\n",
    "for c in classnames:\n",
    "    tmp = where(trainY==c)\n",
    "    for a in tmp[0][0:10]:\n",
    "        print('[{}]: {}'.format(classlabels[trainY[a]], traintxt[a]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-09T16:42:50.706790700Z",
     "start_time": "2023-10-09T16:42:47.703237200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

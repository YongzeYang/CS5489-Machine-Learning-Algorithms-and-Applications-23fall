{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":7112464,"sourceType":"datasetVersion","datasetId":4101275},{"sourceId":7146480,"sourceType":"datasetVersion","datasetId":4125480}],"dockerImageVersionId":30587,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\n%matplotlib inline\nimport matplotlib_inline   # setup output image format\nmatplotlib_inline.backend_inline.set_matplotlib_formats('svg')\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom numpy import *\nfrom sklearn import *\nfrom scipy import stats\nimport csv\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nrandom.seed(100)\nimport pandas as pd\nimport numpy as np\n\nfrom joblib import dump, load\n%matplotlib inline\nimport matplotlib_inline   # setup output image format\nmatplotlib_inline.backend_inline.set_matplotlib_formats('svg')\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom numpy import *\nfrom sklearn import *\nimport os\nimport zipfile\nimport fnmatch\nrandom.seed(100)\nfrom scipy import ndimage\nfrom scipy import signal\nfrom scipy import stats\nimport skimage.color\nimport skimage.exposure\nimport skimage.io\nimport skimage.util\nimport xgboost as xgb\nfrom tqdm import tqdm\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-08T02:09:59.516458Z","iopub.execute_input":"2023-12-08T02:09:59.517003Z","iopub.status.idle":"2023-12-08T02:09:59.533881Z","shell.execute_reply.started":"2023-12-08T02:09:59.516969Z","shell.execute_reply":"2023-12-08T02:09:59.532559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !ls /kaggle/input/nltkdata/nltk_data/\n# !mkdir /usr/local/lib/nltk_data\n# !mkdir /usr/local/lib/nltk_data/corpora\n# !cp -r /kaggle/input/nltkdata/nltk_data/* /usr/local/lib/nltk_data/corpora/","metadata":{"execution":{"iopub.status.busy":"2023-12-08T00:02:00.977759Z","iopub.execute_input":"2023-12-08T00:02:00.978212Z","iopub.status.idle":"2023-12-08T00:02:09.373778Z","shell.execute_reply.started":"2023-12-08T00:02:00.978166Z","shell.execute_reply":"2023-12-08T00:02:09.372232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"concat_dataset = '/kaggle/input/daigt-one-place-all-data/concatenated.csv'\ntest_csv_path = '/kaggle/input/llm-detect-ai-generated-text/test_essays.csv'\n\ntrain_dataset_df = pd.read_csv(concat_dataset)\ntest_dataset_df = pd.read_csv(test_csv_path)\n\ntrain_text_list = train_dataset_df['text'].tolist()\ntest_text_list = test_dataset_df['text'].tolist()\n\ntrain_y = train_dataset_df['generated'].values\n\ntrain_id = train_dataset_df['id'].tolist()\ntest_id = test_dataset_df['id'].tolist()\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T02:10:05.577568Z","iopub.execute_input":"2023-12-08T02:10:05.577955Z","iopub.status.idle":"2023-12-08T02:10:08.571641Z","shell.execute_reply.started":"2023-12-08T02:10:05.577925Z","shell.execute_reply":"2023-12-08T02:10:08.570827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count Vectorizer\nvectorizer = CountVectorizer(ngram_range=(1, 2),max_features=22000)\ntrain_features = vectorizer.fit_transform(train_text_list)\ntest_features = vectorizer.transform(test_text_list)\nprint('CountVectorizer feature representing finished.')\n\n# Calculate TF-IDF value\ntf_trans = feature_extraction.text.TfidfTransformer(use_idf=True, norm='l1')\ntrain_X = tf_trans.fit_transform(train_features)\ntest_X = tf_trans.transform(test_features)\nprint('TF-IDF feature representing calculated.')","metadata":{"execution":{"iopub.status.busy":"2023-12-08T02:10:14.997634Z","iopub.execute_input":"2023-12-08T02:10:14.997993Z","iopub.status.idle":"2023-12-08T02:11:09.500109Z","shell.execute_reply.started":"2023-12-08T02:10:14.997965Z","shell.execute_reply":"2023-12-08T02:11:09.499214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from nltk.corpus import stopwords\n# from nltk import word_tokenize, PorterStemmer, SnowballStemmer, WordNetLemmatizer\n# import re\n# import nltk\n\n# # Init stop words and stemmer\n# stop_words = set(stopwords.words('english'))\n# lemmatizer = WordNetLemmatizer()\n\n# def preprocess_list(str):\n#     \"\"\"\n#     Preprocess text data by removing non-alphanumeric characters & stop word and lemmatizing\n\n#     :param str: input string\n#     :return: preprocessed word list\n#     \"\"\"\n\n#     # Remove non-alphanumeric characters\n#     string_only_alphanumeric = re.sub(r'[^a-zA-Z0-9]', ' ', str)\n#     # Tokenize the string into individual words\n#     words = word_tokenize(string_only_alphanumeric)\n#     new_str = []\n\n#     for word in words:\n#         if not word.isdigit():\n#             # Remove stop word\n#             if word not in stop_words:\n#                 # lemmatization and convert to lower case\n#                 stemmed_word = lemmatizer.lemmatize(word)\n#                 lower_word = stemmed_word.lower()\n#                 new_str.append(lower_word)\n#     return new_str\n\n# test = 'En chikku nange bakra msg kalstiya..then had tea/coffee?'\n# print(preprocess_list(test))\n\n# train_word_list = [preprocess_list(s) for s in train_text_list]\n# test_word_list = [preprocess_list(s) for s in test_text_list]","metadata":{"execution":{"iopub.status.busy":"2023-12-08T00:02:19.808348Z","iopub.execute_input":"2023-12-08T00:02:19.809162Z","iopub.status.idle":"2023-12-08T00:07:27.146723Z","shell.execute_reply.started":"2023-12-08T00:02:19.809102Z","shell.execute_reply":"2023-12-08T00:07:27.145092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import ParameterGrid\n# from gensim.models import Word2Vec\n# from sklearn.metrics import roc_auc_score\n\n# model_w2v = Word2Vec(train_word_list, min_count= 1, vector_size= 500, window= 10)\n\n# def generate_w2v(word_list):\n#     X = []\n#     # Build word2Vec features\n#     for sentence in word_list:\n#         sentence_vector = []\n#         for word in sentence:\n#             # if the word is in the wv list\n#             if word in model_w2v.wv:\n#                 word_vec = model_w2v.wv[word]\n#                 sentence_vector.append(word_vec)\n#         if len(sentence_vector) > 0:\n#             # can be presented as a vector\n#             average_vec = np.mean(sentence_vector,axis=0)\n#         else:\n#             # vector of 0 as default\n#             average_vec = np.zeros(500)\n#         X.append(average_vec)\n#     return X\n\n# train_w2v = generate_w2v(train_word_list)\n# test_w2v = generate_w2v(test_word_list)\n# set_w2v = [train_w2v, test_w2v]\n\n# print(shape(train_w2v))\n# print(shape(test_w2v))","metadata":{"execution":{"iopub.status.busy":"2023-12-08T00:11:34.941244Z","iopub.execute_input":"2023-12-08T00:11:34.941770Z","iopub.status.idle":"2023-12-08T00:15:40.004529Z","shell.execute_reply.started":"2023-12-08T00:11:34.941733Z","shell.execute_reply":"2023-12-08T00:15:40.001787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Count Vectorizer\n# vectorizer = CountVectorizer(stop_words='english', max_features=3150)\n# train_features = vectorizer.fit_transform(train_txt_preprocessed)\n# test_features = vectorizer.transform(test_txt_preprocessed)\n# print('CountVectorizer feature representing finished.')\n\n# # Calculate TF-IDF value\n# tf_trans = feature_extraction.text.TfidfTransformer(use_idf=True, norm='l1')\n# train_X = tf_trans.fit_transform(train_features)\n# test_X = tf_trans.transform(test_features)\n# print('TF-IDF feature representing calculated.')\n\n# train_X = train_w2v\n# test_X = test_w2v\n\n\n\n# clf_rf = ensemble.RandomForestClassifier(random_state=4487,max_depth=8,max_features=0.14000883833708455,n_estimators=1000)\n\n# clf_ada = ensemble.AdaBoostClassifier(random_state=4487,learning_rate=0.03162277660168379, n_estimators=500)\n\n# clf_xgb = xgb.XGBClassifier(objective=\"binary:logistic\", eval_metric='logloss', random_state=4487, use_label_encoder=False,gamma= 0.4257366859173304, learning_rate= 0.35526774997253563, max_depth= 4, n_estimators= 734, subsample= 0.6262634075987297)\n\n# clf_gb = ensemble.GradientBoostingClassifier(n_estimators= 100, learning_rate = 1.0)\n\n# clf_knn = neighbors.KNeighborsClassifier(n_neighbors= 3)\n\n# clf_svm_poly = svm.SVC(C= 3.1622776601683795, degree= 2, kernel='poly',probability=True)\n\n# clf_nb = naive_bayes.BernoulliNB(alpha=1e-10)\n\n\n# clf_list = [('rf', clf_rf), \n#             ('ada', clf_ada), \n#             ('xbg', clf_xgb), \n#             ('gb', clf_gb), \n#             ('knn', clf_knn), \n#             ('svm_poly', clf_svm_poly),\n#             ('nb', clf_nb)]\n\n# clf_list = [('xbg', clf_xgb), \n#             ('knn', clf_knn), \n#             ('svm_poly', clf_svm_poly),\n#             ('nb', clf_nb)]","metadata":{"execution":{"iopub.status.busy":"2023-12-08T01:27:31.293794Z","iopub.execute_input":"2023-12-08T01:27:31.294205Z","iopub.status.idle":"2023-12-08T01:27:31.304369Z","shell.execute_reply.started":"2023-12-08T01:27:31.294172Z","shell.execute_reply":"2023-12-08T01:27:31.303175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train Model\n# model = ensemble.VotingClassifier(estimators=clf_list, voting='soft')\nmodel = naive_bayes.BernoulliNB(alpha=1e-10)\nmodel.fit(train_X,train_y)\nprint('Trained.')\n\ny_pred = model.predict_proba(test_X)\nprint('Predicting finished')","metadata":{"execution":{"iopub.status.busy":"2023-12-08T02:11:21.459663Z","iopub.execute_input":"2023-12-08T02:11:21.460060Z","iopub.status.idle":"2023-12-08T02:11:21.811775Z","shell.execute_reply.started":"2023-12-08T02:11:21.460015Z","shell.execute_reply":"2023-12-08T02:11:21.811008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_df = pd.DataFrame({\n    'id': test_id,\n    'generated': y_pred[:, 1]\n})\n\n\noutput_df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T22:45:36.591162Z","iopub.execute_input":"2023-12-05T22:45:36.591528Z","iopub.status.idle":"2023-12-05T22:45:36.599753Z","shell.execute_reply.started":"2023-12-05T22:45:36.591500Z","shell.execute_reply":"2023-12-05T22:45:36.598887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import accuracy_score, roc_curve, roc_auc_score, confusion_matrix, f1_score, precision_score\n\n# predY = model.predict(test_X)\n# predY_proba = y_pred\n\n# # Calculate metrics data\n# accuracy = accuracy_score(testY, predY)\n# auc = roc_auc_score(testY, predY_proba[:, 1])\n# f1 = f1_score(testY, predY)\n# precision = precision_score(testY, predY)\n\n# tn, fp, fn, tp = confusion_matrix(testY, predY).ravel()\n# sensitivity = tp / (tp + fn)\n# specificity = tn / (tn + fp)\n\n# metrics_data.append([name, accuracy, auc, f1, precision, sensitivity, specificity])\n\n# # Show metrics data\n# print(f\"Result for Voting Classifier:\")\n# print(\"    Accuracy: \", accuracy)\n# print(\"    AUC: \", auc)\n# print(\"    F1 Score: \", f1)\n# print(\"    Precision: \", precision)\n# print(\"    Sensitivity: \", sensitivity)\n# print(\"    Specificity: \", specificity)","metadata":{},"execution_count":null,"outputs":[]}]}